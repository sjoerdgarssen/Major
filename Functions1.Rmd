---
title: "PLS-DA"
author: "Sjoerd Garssen"
date: "02/09/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#activa packages needed
library(dplyr)
library(ggplot2)
library(ggpubr)
library(ggforce)
library(tidyr)
```

```{r}
#This is the PLS-DA function. It makes the PLS-model, and stores regression coefficient, scores, loadings and weights. Besides this, the output also contains mean and sd per X-variable, such that a new sample can be preprocessed and predicted.
plsda_function<-function(X,c,A){
  #X: matrix X or a data frame of X
  #c: class label vector or column
  #A: #PLS components to use in the model.
  
  #make matrices of the input 
  X=as.matrix(X)
  c=as.matrix(c)

  #autoscale X. Alternative function; scale(). But in this way, the mean and sd's are kept, needed for preprocessing unseen test data when needed. 
  mean=X%>%apply(2, mean)
  sd=X%>%apply(2, sd)
  length=dim(X)
  for (i in 1:length[2]){ 
    X[,i]<-(X[,i]-mean[i])/sd[i]
  }
  
  #mean centre c. Nothing changes when class balance is present, but it is needed when there is a class imbalance.
  meanc=mean(c)
  c=c-meanc
  
  #for loop for the different PLS components. NIPALS PLS1 algorithm:
  for (i in 1:A){
    #calculate PLS weight w
    w=t(X)%*%c
    
    #calculate scores T
    t=X%*%w/sqrt(sum(w^2))
    
    #calculate loadings P
    p=t(t)%*%X/sum(t^2)
    
    #transpose P because it is mostly notated as the transposed version of the P calculated one line above here.
    p=t(p)
    
    #calculate c loading Q (scalar)
    q=t(c)%*%t/sum(t^2)
    if (i==1){
      #Make it ready for additions made with the next PLS component. So here every loading, weights and scores are converted to matrix or vector, to be able to add the loadings, weights and scores of next components.
      W=w
      Scores=t
      P=p
      Q=q
    } else {
      #add the loadings etc. of the next PLS component to the matrix of the previous components. 
      W=cbind(W,w)
      Scores=cbind(Scores,t)
      P=cbind(P,p)
      Q=rbind(Q,q)
    }
    
    #calculate the X_resid and the c_resid by subtracting information of the last made component
    X=X-t%*%t(p)
    c=c-t%*%q}
  #calculate regression coefficient.
  b=W%*%solve(t(P)%*%W)%*%Q 

  #make a list of all scores, loadings etc. to return after the function. Means and sd are needed to preprocess a new sample before making a prediction.
  list<-list(PLSweigths=W,Scores=Scores,Xloading=P,Yloading=Q,regressioncoefficient=b, means=mean, sd=sd)
  return(list)}
```

```{r}
#Makes partition function needed for k-fold CV. Class balance kept the same in all partitions. The result is a vector that can be added as extra column to the data set, to make filtering a subset out easy.
partition <- function(k, f) {
  #k: K in K-fold CV. The number of subsets the data has to be split into.
  #f: the class label vector, such that every subset get equal proportions of the classes.
  if (is.atomic(f)) {
    lenf <- length(f)
  } else {
    lenf <- length(f[[1]])
  }
  part <- vector(mode="integer", length=lenf)
  tapply(X=1:length(part), INDEX=f, FUN=function(i) {
    part[i] <<- as.integer((sample(1:length(i)) + sample(1:k,1))%%k + 1)
  }
  )
  return(part)
}
```

```{r}
#function to predict new samples with a model made with plsda_function()
prediction<-function(X,m){
  #X: data set 
  #m: output of plsda_function()
  X=as.matrix(X) #make a matrix of X. This ensures that rest of the function works.
  
  #Preprocess the sample(s) that need(s) to be predicted. Autoscaling with the mean and sd of the training set (the set which was used to build pls-da model)
  for (i in 1:ncol(X)){
    X[,i]<-(X[,i]-m$means[i])/m$sd[i]
  }
  #calculate the predicted c and return it as output of this function.
  c_pred<-X%*%m$regressioncoefficient
  return(c_pred)
}
```

```{r}
DQ2=function(c,pred){
  #c: true class 
  #pred: predicted class
  c_mean=mean(c)
  
  #DQ2 calculation
  TSS=sum((c-c_mean)^2) #Total sum of squares
  #get index of samples with ctrue=1 and ctrue=-1. pressd uses all samples. In this function the pressd is calculated first for samples belonging to class1 and thereafter for samples beloning to class 2. This is because only predictions that are not beyond the class label contribute to the DQ2. pressd: the well known press, but than the adapted version needed for DQ2 (not considering predictions beyond class label)
  index1=which(c==1)
  index2=which(c!=1)
  PRESSD=0
  for (d in 1:length(index1)){ #samples belonging to group 1
    if (pred[index1[d]]<1){ #only contribute to PRESSD if c_pred is below 1.
      pressd=(c[index1[d]]-pred[index1[d]])^2
      PRESSD=PRESSD+pressd
    } else {
      PRESSD=PRESSD
    }
  }
  for (d in 1: length(index2)){ #samples belonging to group 2)
    if (pred[index2[d]]>-1){ #only contribute to PRESSD if c_pred is above -1.
      pressd=(c[index2[d]]-pred[index2[d]])^2
      PRESSD=PRESSD+pressd
    } else {
      PRESSD=PRESSD
    }
  }
  DQ2=1-(PRESSD/TSS) #calculate DQ2 and return it as output of this function.
  return(DQ2)
}
```

```{r}
#function to calculate RMSECV/RMSEP.
RMSE<-function(pred,c){
  #pred: predicted class vector (numerical)
  #c: class label vector
  error=sqrt(sum((c-pred)^2)/length(c))
  return(error)
}
```

```{r}
#which percentage of group B (healthy, control) is correctly predicted. Specificity calculation.
specificity<-function(pred,c,threshold=0){
  #pred: predicted class label (numerical). For example: the output of prediction() function.
  #c: class label vector.
  #threshold: A number, default set to 0.
  TN=0 #True Negative
  predclass=ifelse(pred>threshold,1,-1)
  cN=c[c==-1] #All negatives. group -1 is seen as negative here. (control group)
  predclassN=predclass[c==-1] #get all predictions of the samples belonging to the case group. So the predictions of all negatives.
  for (i in 1:length(cN)){
    if (predclassN[i]==cN[i]){
      TN=TN+1
    }
    else {
      TN=TN
    }
  }
  sp=100*TN/length(cN) #calculate specificity
  return(sp)
}
```

```{r}
#Cancer:1, healthy:-1. Same function as specificity, but then calculating sensitivity instead of specificity.
sensitivity<-function(pred,c,threshold=0){
  #pred: numerical predicited class 
  #c: class label vector
  #threshold: a number. Default set to 0.
  TP=0 #true positive
  predclass=ifelse(pred>threshold,1,-1)
  cP=c[c==1] #get all positives.
  predclassP=predclass[c==1]
  for (i in 1:length(cP)){
    if (predclassP[i]==cP[i]){
      TP=TP+1
    }
    else{
      TP=TP
    }
  }
  sn=100*TP/length(cP) #calculate Sensitivity. cP; all positives.
  return(sn)
}
```

```{r}
#accuracy function
acc<-function(c,pred, threshold=0){
  #c: class label vector
  #pred: predicted class vector (numerical)
  #threshold: should be a number. 0 is default, but others can be applied too.
  good=0
  predclass=ifelse(pred>threshold,1,-1)
  for (i in 1:length(c)){
    if (predclass[i]==c[i]){
      good=good+1
    }
    else {
      good=good
    }
  }
  acc=100*good/length(c)
  return(acc)
}
```

```{r}
#threshold function with normal distribution assumption
threshold<-function(c,pred){
  #c: true class
  #pred: predicted class
  
  #select all predicted values for the individuals who truly belong to group A (c=1). Calculate mean and sd of the prediction needed for the pnorm.
  predA=pred[c==1]
  A_mean=predA%>%mean()
  A_sd=predA%>%sd()
  
  #Now for group B the same story/
  predB=pred[c==-1]
  B_mean=predB%>%mean()
  B_sd=predB%>%sd()
  
  #get x-values for the pnorm.
  x=seq(-2,2,by = .01)
  y_A=pnorm(x, mean = A_mean, sd = A_sd)
  y_B=pnorm(x, mean = B_mean, sd = B_sd, lower.tail = FALSE)
  threshold=(x[length(y_B[y_B>y_A])]+x[length(y_B[y_B>y_A])+1])/2 #give a close estimate of the intercept of the distribution of predicted classes belonging to group A and the similar distribution of group B. Return this as output of this function.
  return(threshold)
}
```

```{r}
#Cross Validation function. Uses K-fold CV
CV<-function(X,k=5,c,A, ss=1, threshold=0){
  #X: data set to use CV with
  #c: class label belonging to X
  #k: k of k-fold CV you want to execute
  #A: maximum number of components that is wanted to test
  #ss: Set Seed. To be able to use repeated CV in a later function. Also it is used to deliver constant results when using non-repeated CV.
  #threshold: can be 0 or distribution based (indicated by threshold='dist', 'dis' or 'distribution')
  
  #make partition
  set=as.data.frame(X) #make dataframe of X and copy it with the name 'set' (derived from 'data set'). This set gets the extra columns of partition function and the class label. This makes selecting subsets for the CV easier.
  c=as.matrix(c) #make matrix of class label. 
  set.seed(ss) #to deliver constant results of partition function. Is also used to be able to do repeated CV (later function). Changing set.seed is then CV with other partioning. 
  set$part<-partition(k, c)
  set$c<-c
  
  #make data frame to store RMSECV etc. belonging to each number of components included in a model.
  df=data.frame(LV=1:A, RMSECVs=NA, acc=NA, NMC=NA, DQ2=NA)
  #loop
  for (a in 1:A){ #A: max number of components to be tested. 
    c_hat<-vector() #predicted c value
    c_hatclass<-vector() #predicted class (so a threshold is used)
    c_true<-vector() #true class
    for (i in 1:k){ #to select subset as validation/test set in CV.
      #split new training and test set
      if (k==nrow(X)){ #LOOCV
        training=set%>%slice(-i)
        test=set[i,]
      } else {
        training=set%>%filter(part!=i)
        test=set%>%filter(part==i)
      }
    
      #remove partition and class column to create the X of training set.
      training_X=training[,1:ncol(X)]%>%as.matrix()
      #make class column of training
      training_c=training$c%>%as.matrix()
    
      #make test X
      test_X=test[,1:ncol(X)]%>%as.matrix()
    
      #make test true class column
      test_c=test$c%>%as.matrix()
      
      #append true class column to the previous one(s). This is later on also done for c_hat and c_hatclass. In this way all these 3 vectors have the same order. Now for example c_hat starts with sample 14, just as c_hatclass and c_true.
      c_true<-c_true%>%append(test_c)
    
      #make model on training set
      model<-plsda_function(training_X,training_c,A=a)
    
      #predict test set using a threshold of 0.
      tres=0
      pred<-prediction(test_X,model) #numerical prediction
      pred_class<-ifelse(pred>tres,1,-1) #class prediction calculated with threshold
      c_hat<-c_hat%>%append(pred) #numerical prediction
      c_hatclass<-c_hatclass%>%append(pred_class) #class prediction
    }
    #calculate RMSECVs of the different number of LVs selected
    RMSECV<-sqrt(sum((c_hat-c_true)^2)/nrow(X))
    #store them per #LVs.
    df[a,2]<-RMSECV
    
    #if a threshold is wanted based on the distribution function instead of 0:
    if (threshold%in%c('dist','distribution','dis')){
      tresd=threshold(c=c_true,pred=c_hat)
      c_hatclass=ifelse(c_hat>tresd,1,-1)
    }
    else {
      c_hatclass=c_hatclass
    }
    
    
    #needed for accuracy and NMC calculation
    good=0
    NMC=0
    for (m in 1:length[1]){
      if (c_true[m]==c_hatclass[m]){
        good=good+1
        NMC=NMC
      }
      else {
        good=good
        NMC=NMC+1
      }
    }
    
    #store accuracy and nmc. Also calculate DQ2 and store.
    acc=100*good/nrow(X)
    df[a,3]<-acc
    df[a,4]=NMC
    df[a,5]=DQ2(c=c_true,pred=c_hat) 
  }
  plot<-df%>%ggplot(aes(x=LV,y=RMSECVs))+geom_line()
  list=list(performance=df,RMSECV_plot=plot) #return the dataframe with the different metrics per number of components. Also return the traditional RMSECV vs #components plot.
  return(list)  
}
```

```{r}
#bootstrap sampling function. Randomly draw N samples from X. 
BTS_sampling<-function(X,c, ss=1){
  #X: data set
  #c: class label belonging to X
  #ss: used for set.seed(). To be able to run multiple bootstrap samplings and to deliver constant results.
  
  #make matrices of input
  X<-as.matrix(X)
  c<-as.matrix(c)
  
  #draw N samples at random with replacement. Set.seed() for constant results.
  set.seed(ss)
  sampling=sample(1:nrow(X),replace=TRUE, size=nrow(X))
  
  #copy X and c as skeleton for the BTS_X and BTS_c. Every row will be changed by the sampling, but the design is kept the same in this manner
  BTS_X<-X
  BTS_c<-c
  
  #Make bootstrap sample
  for (i in 1:nrow(X)){
    BTS_X[i,]=X[sampling[i],]
    BTS_c[i]=c[sampling[i]]}
  
  #make a list of BTS_X and BTS_c for the output of the function
  list<-list(BTS_X=BTS_X,BTS_c=BTS_c, sample=sampling)
  
  #returns bootstrap sample (BTS_X) with their classes (BTS_c). Also the sampling (N randomly drawn numbers between 1 and N, with replacement) is stored, needed for other functions.
  return(list)
  }
```

```{r}
#make function to sample N samples from the large data set (most often used in this study: N=50)
smallsample<-function(X,c,n=50,ss=1,bal=1){
  #X: data
  #c: class label vector
  #n=size of small set
  #ss=set.seed value
  #bal=A:B balance/ratio. 
  
  X=as.matrix(X)
  c=as.matrix(c)
  
  #Calculate number of samples needed of group A and of group B with the wanted class (im)balance
  nA=n*bal/(bal+1) #number of samples of group A/1 (case)
  nB=n-nA #number of samples of group B/-1 (control)
  
  #bind X and c to be able to distinguish on class for sampling
  Xc=cbind(X,c)
  XA=Xc[Xc[,ncol(Xc)]==1,]
  XB=Xc[Xc[,ncol(Xc)]==-1,]
  #calculate how many samples of group A and B are present in the original large data set
  length_A=c[c==1]%>%length()
  length_B=c[c==-1]%>%length()
  
  #get random numbers to get samples later
  set.seed(ss)
  sample_A=sample(c(1:length_A),replace=FALSE, size=nA)
  set.seed(ss+1)
  sample_B=sample(c(1:length_B),replace=FALSE, size=nB)
  
  #get the X and c of the N drawn samples into a list called small
  Xa=XA[sample_A,]
  Xb=XB[sample_B,]
  Xsmall=rbind(Xa,Xb)
  small=list(Xsmall=Xsmall[,-ncol(Xsmall)],csmall=Xsmall[,ncol(Xsmall)])
  
  #get the large rest dataset. So these are the samples that are not drawn from the large set into the small set.
  restA=XA[c(1:length_A)%in%sample_A==FALSE,]
  restB=XB[c(1:length_B)%in%sample_B==FALSE,]
  Xlarge=rbind(restA,restB)
  large=list(Xlarge=Xlarge[,-ncol(Xlarge)],clarge=Xlarge[,ncol(Xlarge)])
  
  #store small and large datasets into a list for the output
  list=list(small=small,large=large)
  
  return(list)
}
```

```{r}
#create permuted c
c_random<-function(c,ss=1){
  #c: class label vector that is wanted to be randomized
  #ss: used for constant results. And to be able to repeat permutation testing.
  
  crandom=c
  length=length(c)
  set.seed(ss)
  random_index=sample(1:length,replace = FALSE, size=length)
  for (i in 1:length){
    crandom[random_index[i]]=c[i]
  }
  return(crandom)
}
```

```{r}
#angle calculation between two vectors. Needed for the model selection method 'Angle between regression coefficients'.
angle<-function(v1,v2){
  #v1: first vector. Here: regression coefficient with q components
  #v2: second vector. Here; regression coefficient with q+1 components.
  v1=as.matrix(v1)
  v2=as.matrix(v2)
  lv1l=sqrt(sum(v1^2)) #get the square root of the sum of the squares of vector 1.
  lv2l=sqrt(sum(v2^2)) #get the square root of the sum of the squares of vector 2.
  angle=acos((t(v1)%*%v2)/(lv1l*lv2l)) #calculate angle (is in radians instead of degrees)
  angle=angle*180/pi #convert to degrees.
  return(angle)
}
```

```{r}
#Function to calculate the angle between regression coefficients for every component between 1 and the maximum tested.
LV_select_Bangle<-function(X,c,A=15){
  #X: data set
  #c: class label vector
  #A: maximum number of components to be tested.
  matrix=matrix(NA,nrow=ncol(X),ncol=A) #to store regression coefficients for every number of components between 1 and A.
  bangle=data.frame(A=c(1:(A-1)),angle=NA) #to store the angles between the regression coefficients.
  for (a in 1:A){
    #make model with every number of components between 1 and A, and store regression coefficient
    model=plsda_function(X=X,c=c,A=a)
    matrix[,a]=model$regressioncoefficient
    if (a>1){
      #calculate angle between model with a-1 and a components
      bangle[(a-1),2]=angle(v1=matrix[,(a-1)],v2=matrix[,a])
    } 
  }
  return(bangle)
}
#LV_select_Bangle(X=smallx,c=smallc,A=15)
```

```{r}
#B angle as feature selection method, but not on small data set, but on repeated parts of small data set. Default is set that these parts contain 48 samples, so 2 samples are left out. This function is used for the LV_select_r() function, which detect the maximum number of components.
LV_select_Bangle_r<-function(X,c,A=15,r,n=48,bal=1){
  #X: data set
  #c: class label vector belonging to X
  #A: maximum number of components to be tested.
  #r: number of repeats. How many times LV_select_Bangle() needs to be executed on a data subset.
  #n: how many samples form a subset, that will be used in LV_select_Bangle() function. Default set to 48, so leaving 2 samples out when having a 50 sample data set.
  #bal: the class balance of the subset. default set to class balance, so bal=1.
  matrix=matrix(NA,nrow=r,ncol=(A-1)) #to store the angles of the different repeats, belonging to different number of components.
  df=data.frame(A=1:(A-1),mean=NA,sd=NA) #to store means and sd's of the above made matrix
  for (i in 1:r){
    small=smallsample(X=X,c=c,n=n,ss=i,bal=bal)
    smallx=small[['small']][['Xsmall']]
    smallc=small[['small']][['csmall']]
    LVdata=LV_select_Bangle(X=smallx,c=smallc,A=A)
    matrix[i,]=LVdata$angle
  }
  mean=matrix%>%apply(2,mean)
  sd=matrix%>%apply(2,sd)
  df$mean=mean
  df$sd=sd
  
  list=list(df=df,matrix=matrix)
  return(list)
}
```

```{r}
#Function to calculate 0.632 bootstrap RMSE and ACC. BTS=bootstrap, pred=prediction, 2=2 metrics (RMSE and ACC)-->BTS_pred2().
BTS_pred2<-function(X, c, r, A){
  #X: data set
  #c: class label vector belonging to X
  #r: number of bootstrap repeats
  #A: the optimal number of components.
  
  #make vector for storing 0.632 error
  vector.632<-vector() #vector to store 0.632 RMSE error of every bootstrap repeat
  vector.acc.632<-vector() #vector to store 0.632 ACC of every bootstrap repeat
  
  #r bootstrap repeats
  for (i in 1:r){
    #constant results
    set.seed(i)
    
    #Make bootstrap sample
    BTS<-BTS_sampling(X, c, ss=i)
    
    #Make bootstrap model
    model<-plsda_function(BTS$BTS_X, BTS$BTS_c, A)
    
    #Make bootstrap test set
    test<-c(1:nrow(X))%in%BTS$sample==FALSE
    X_test<-X[test,]
    c_test<-c[test]
    
    #Predict c of test set
    pred_new<-prediction(X_test,m=model)
    #calculate accuracy on test set
    acc_test=acc(c=c_test,pred=pred_new)
    
    #calculate prediction error as RMSEP of test set
    error_new=RMSE(pred=pred_new,c=c_test)
    
    #predict c of training samples
    trainx=X[BTS$sample,]
    trainc=c[BTS$sample]
    pred_train=prediction(X=trainx,m=model)
    acc_train=acc(c=trainc,pred=pred_train) #calculate accuracy on training samples
    error_train=RMSE(pred=pred_train,c=trainc) #calculate RMSE on training samples
    
    #calculate 0.632 error and store
    err.632=0.632*error_new+(1-0.632)*error_train
    vector.632<-vector.632%>%append(err.632)
    acc.632=0.632*acc_test+(1-0.632)*acc_train
    vector.acc.632=vector.acc.632%>%append(acc.632)
  }
  #calculate mean and sd 0.632 error (RMSE and ACC)
  mean<-mean(vector.632)
  sd<-sd(vector.632)
  accmean<-mean(vector.acc.632)
  accsd<-sd(vector.acc.632)
  
  #store everything as list for the output 
  list<-list(vector.632=vector.632,mean=mean,sd=sd,vector.acc.632=vector.acc.632,accmean=accmean,accsd=accsd)
  return(list)
}
```

```{r}
#bootstrap error vs A. Needed for the LV_select_r() function when using bootstrap model selection method.
BTS_LV_select<-function(X,c,r,A,metric='RMSE',threshold=0){
  #X: data set
  #c: class label vector belonging to X
  #r: number of bootstrap repeats
  #metric: possibilities: 'RMSE' and 'ACC'. 'RMSE' is used as default.
  BTS_ma=matrix(NA,ncol=A,nrow=r) #to store all values of the different bootstrap repeats belonging to different number of components.
  for (a in 1:A){
    bts=BTS_pred2(X=X,c=c,r=r,A=a)
    if (metric=='RMSE'){
      BTS_ma[,a]=bts$vector.632 
    }
    else if (metric=='ACC'){
      BTS_ma[,a]=bts$vector.acc.632
    }
  }
  return(BTS_ma)
}
```

```{r}
#functions for mean centring
#mean centre X
mcX=function(X){
  mean=X%>%apply(2,mean)
  sd=X%>%apply(2,sd)
  for (i in 1:ncol(X)){
    X[,i]<-(X[,i]-mean[i])/sd[i]
  }
  list=list(X=X,mean=mean,sd=sd)
  return(list)
}
#mean centre c
mcc=function(c){
  mean=mean(c)
  c=c-mean
  return(c)
}
#preprocess new data with the mean and sd of the known data.
mc_new=function(X,Xmean,Xsd){
  #X: data set to be processed
  #Xmean: mean of X that was used to determine the means and sds of every variable. (training set for example)
  #Xsd: sd^
  for (i in 1:ncol(X)){
    X[,i]=(X[,i]-Xmean[i])/Xsd[i]
  }
  return(X)
}
```

```{r}
#get selectivity ratio of the variables
SR<-function(X,model){
  #X: The data set used to make the model
  #model: The model used to calculate SR.
  
  SR=X[1,] #to get the same design of the data set, with same column names etc.
  SR=ifelse(SR==0,NA,NA) #change everything to NA values, such that not by coincidence other values are used caused by the previous line.
  SSxp=SR #Make dataframe with eXPlained sum of squares. 
  SSe=SR #make dataframe with residual sum of squares.
  X=mc_new(X,Xmean=model$means,Xsd=model$sd) #autoscale X
  blength=sqrt(sum(model$regressioncoefficient^2)) #calculate length of regression coefficient, needed for normalisation to length 1 in the next line.
  tTP=X%*%model$regressioncoefficient/blength #calculate the tTP vector.
  tTPtTP=t(tTP)%*%tTP #calculate tTP'tTP, needed to calculate pTP later.
  tTPtTP=tTPtTP[1,1] #to be able to do calculations. Now it is a scalar, otherwise R sees it as a matrix.
  pTP=t(X)%*%tTP/tTPtTP #calculate pTP
  E=X-(tTP%*%t(pTP))#calculate residuals
  for (i in 1:nrow(pTP)){ #per variable calculate SR
    SSxp[i]=sum((tTP%*%t(pTP[i,]))^2)
    SSe[i]=sum(E[,i]^2)
    SR[i]=SSxp[i]/SSe[i]
  }
  df=data.frame(SSexp=SSxp,SSres=SSe,SR=SR) #store the explained, unexplained and fraction.
  return(df)
}
```

```{r}
#function to calculate VIP scores.
VIP<-function(X,model){
  #X: data set used to build the model
  #model: Based on this model the VIP scores will be calculated.
  q=model$Yloading
  Scores=model$Scores
  W=model$PLSweigths
  #normalise W to length 1
  for (A in 1:(ncol(W))){
    length=sqrt(sum(W[,A]^2))
    W[,A]=W[,A]/length
  }
  VIP=data.frame(Var=c(1:ncol(X)),VIP=NA) #to store VIP for every variable
  for (j in 1:ncol(X)){
    uppt=0 #upper part of the formula
    downt=0 #down part of the formula
    for (a in 1:ncol(Scores)){
      upp=(q[a]^2%*%t(Scores[,a])%*%Scores[,a])*(W[j,a]^2)
      uppt=uppt+upp
      down=(q[a]^2%*%t(Scores[,a])%*%Scores[,a])
      downt=downt+down
    }
    VIP[j,2]=sqrt((ncol(X)*uppt)/downt)
  }
  return(VIP)
}
```

```{r}
#get n most IMPortant variables based on regression coefficient/SR/VIP
imp<-function(X,model,n=10,method='SR'){
  #X: the data set which is used to build a model
  #model: model build with X
  #n: The top N variables you want to be returned. 
  #method: possibilities: 'SR','VIP','B'
  if (method=='SR'){
    sr=SR(X,model)
    sr=sr$SR
    sor=sr%>%sort(decreasing=TRUE,index.return=TRUE)
  }
  else if (method=='B'){
    df=data.frame(col=colnames(X),B=model$regressioncoefficient)
    sor=df$B%>%sort(decreasing=TRUE,index.return=TRUE)
  }
  else if (method=='VIP'){
    df=VIP(X,model)
    sor=df$VIP%>%sort(decreasing = TRUE,index.return=TRUE)
  }
  col=colnames(X)
  col=col[sor$ix]
  imp=col[1:n]
  return(imp) #return the Top N variable names.
}

```

```{r}
#make repeats of 1CV with whole dataset (or whole small set) and determine mean and sd RMSECV per A. (or other metric)
#this is needed to be able to select the optimum number of components when doing repeated 1cv.
r1cvperformance<-function(X,c,k,A,r,method='RMSECV',threshold=0){
  #X: data set
  #c: class label vector beloning to X
  #k: the k in k-fold 1CV
  #A: maximum number of components to be tested in model selection
  #r: number of repeats in repeated 1CV
  #method: possibilities: 'RMSECV','DQ2','NMC'
  #threshold: the threshold options that can be used in CV() function.
  
  #Output of cv function is RMSECVs instead of RMSECV
  if (method=='RMSECV'){
    method='RMSECVs'#RMSECV values from CV() function were stored as RMSECVs. Therefore, this additional line was needed. NMC and DQ2 were called NMC and DQ2, so no adaption was needed when using one of these two methods.
  } 
  else {
    method=method
  }
  #make matrix to store every single value
  #make df to store mean and sd of every component
  matrix=matrix(NA,nrow=r,ncol=A)
  df=data.frame(A=1:A,mean=NA,sd=NA)
  for (i in 1:r){ #repeat 1CV and store performance.
    cv=CV(X=X,k=k,c=c,A=A,ss=i,threshold)
    matrix[i,]=cv[['performance']][[method]]
  }
  mean=matrix%>%apply(2,mean)
  sd=matrix%>%apply(2,sd)
  df$mean=mean
  df$sd=sd
  list=list(df=df,matrix=matrix)
  return(list)
}
```

```{r}
#Latent Variable selection repeated
#function to select optimal number of components. Use paired t.test to test whether an additional component is significantly better than a model with the previous number of components. To be able to do this, the performance of different repeats of for example 1CV is needed.
#Look to component a, a+1 and a+2. If a+1 is better than a, then accept a+1. If this is not the case, but a+2 is better than a, then accept a+2. Take repeated k-fold cv and take the means for Aopt selection.
LV_select_r<-function(X,k=5,c,A,r,method,n=48,bal=1,threshold=0){
  #X: data set
  #c: class label vector belonging to X
  #k: k in k-fold 1CV (only used when method is a 1CV method)
  #A: maximum number of components to test
  #r: number of repeats in 1CV, bootstrap or B-angle.
  #method: options: 1cv methods: 'RMSECV', 'NMC' or 'DQ2'. Bootstrap methods: 'BTS' or 'BTSacc'. 'Bangle'.
  #n: only used when using B-angle method. 48 is default, so 2 samples are left out in every repeat. Samples need to be left out, otherwise repeating the process is not possible and paired t test cannot be executed.
  #bal: balance of the data sets. Only used when using B-angle. 
  #threshold: same options can be used as in the CV() function.
  
  if (method%in%c('RMSECV','NMC','DQ2')){ #if 1cv method is used to select optimum number of components. Per number of components, multiple performance estimates need to be present, otherwise no paired t test can be executed. So this is the output of the r1cvperformance() function, as described earlier. For more information, see that function.
    df=r1cvperformance(X=X,c=c,k=k,A=A,r=r,method=method,threshold = threshold) 
    df=df$matrix
  }
  else if (method%in%c('Bangle','angle')){ #if model selection is based on angle between regression coefficient, you need to use the output of LV_select_Bangle_r. This output is similar as with r1cvperformance() but then not with 1cv.
    df=LV_select_Bangle_r(X,c,A,r,n,bal)
    df=df$matrix
  }
  else if (method%in%c('BTS','BTSerror','bts')){ #if model selection is based on the bootstrap RMSE error. BTS_LV_select() gives similar output as the functions used for 1cv or b-angle.
    df=BTS_LV_select(X,c,r,A)
  }
  else if (method%in%c('BTSacc','BTS ACC','btsacc')){ #same as with BTS RMSE error, but then with the BTS Accuracy error metric.
    df=BTS_LV_select(X,c,r,A,metric='ACC')
  }
  LV=1
  if (method%in%c('RMSECV','Bangle','NMC')){ #select optimum number of components based on RMSECV, NMC or B-angle.
    dfmeans=df%>%apply(2,mean)
    for (i in 3:A){
      a=i-1
      b=i-2
      if (dfmeans[b]>dfmeans[a]){ #investigate whether q+1 components has a lower error than model with q components. If this is true, then execute a paired t.test(one-sided). If p-value is smaller than 0.05, q+1 components is accepted as Aopt. However, when q+1 has a higher error compared to q components, q+2 components will also be investigated when using RMSECV or NMC as metric (so not B-angle). When q+2 components has a lower error, a paired t.test will be executed. When the resulting pvalue is below 0.05, q+2 components is accepted.  It stops when no additional component is selected. After testing q+1, q+2 will be tested in exactly the same way. So this goes on until no additional component is accepted. 
        t=t.test(df[,a],df[,b],paired=TRUE,alternative='less')
        if (t$p.value<0.05){
          LV=a
        }
        else {
          break
        }
      }
      else {
        if (method%in%c('Bangle','bangle') && i==15){
          break
        }
        else if (dfmeans[b]>dfmeans[i]){
          t=t.test(df[,i],df[,b],paired=TRUE,alternative='less')
          if (t$p.value<0.05){
            LV=i
          }
          else {
            break
          }
        }
        else {
          break
        }
      }
    }
    
    if (method%in%c('Bangle','angle')){ #With B-angle not 1 component can be accepted, as it is always the angle between at least component 1 and 2.
      LV=LV+1
    }
  }

  else if (method%in%c('DQ2','dq2')){ #exactly the same story as with RMSECV and NMC, but now the lower error is indicated by a higher DQ2, so al rules are upside down here.
    dfmeans=df%>%apply(2,mean)
    for (i in 3:A){
      a=i-1
      b=i-2
      if (dfmeans[b]<dfmeans[a]){
        t=t.test(df[,a],df[,b],paired=TRUE,alternative = 'greater')
        if (t$p.value<0.05){
          LV=a
        }
        else {
          break
        }
      }
      else {
        if (dfmeans[b]<dfmeans[i]){
          t=t.test(df[,i],df[,b],paired=TRUE,alternative='greater')
          if (t$p.value<0.05){
            LV=i
          }
          else {
            break
          }
        }
        else {
          break
        }
      }
    }
  }
  else if (method%in%c('BTS','bts')){ #same as with RMSECV, but now q+2 will not be considered when q+1 is not significantly better than q components. 
    dfmeans=df%>%apply(2,mean)
    for (i in 2:A){
      a=i-1
      if (dfmeans[a]>dfmeans[i]){
        t=t.test(df[,i],df[,a],paired=TRUE,alternative='less')
        if (t$p.value<0.05){
          LV=i
        }
        else {
          break
        }
      }
      else {
        break
      }
    }
  }
  else if (method%in%c('BTSacc','BTS ACC')){ #same as with BTS above, but now with BTS accuracy. Same rules are used, except that everything is upside down here compared to BTS RMSE.
    dfmeans=df%>%apply(2,mean)
    for (i in 2:A){
      a=i-1
      if (dfmeans[a]<dfmeans[i]){
        t=t.test(df[,i],df[,a],paired=TRUE,alternative = 'greater')
        if (t$p.value<0.05){
          LV=i
        }
        else {
          break
        }
      }
      else {
        break
      }
    }
  }
  return(LV)
}
```

```{r}
#double cross validation function
DCV<-function(X,c,dK,sK,A, ss=1, method='RMSECV',threshold=0,r=1){
  #X: data set
  #c: class label belonging to X
  #dK: k in K-fold outer CV loop. (dK derived from 'double k-fold cross validation')
  #sK: k in k-fold inner cv loop. (sK derived from 'single k-fold cross validation')
  #A: maximum number of components to be tested in the inner loop.
  #ss: to be able to do repeated DCV and repeating the function leads to the same results.
  #method: the method to use in model selection in the inner loop. The same options can be used here as in LV_select_r() function.
  #threshold: which threshold to use in the calculation of Accuracy, sensitivity and specificity. Input can be 0 or the same as mentioned in the CV() function
  #r: the number of repeats that is wanted for the inner loop model selection. So the r argument in the LV_select_r() function.
  
  #make partition
  s=as.data.frame(X) #make copy of X and call it s. This is similar as what was done with 'set' in the CV() function. s is used to add the partition and class label colums to simplify leaving out a subset of the data.
  c=as.matrix(c)
  set.seed(ss+1) #again this leads to constant results. The +1 is randomly chosen.
  #partitioning for outer CV. After these 2 lines s contains all columns of X + partition column + class column.
  s$part<-partition(dK, c)
  s$c<-c
  length=dim(X)
  
  #vector to store accuracy values of different outer CV loops.
  joo=vector()
  
  #these vectors are made with same intentions as CV() function. See CV() function for further explanation.
  c_hat_abs=vector() 
  c_hat<-vector() 
  c_true<-vector() 
  
  #outer loop
  for (j in 1:dK){
    #LOOCV 
    if (dK==nrow(X)){
      training=s%>%slice(-j)
      test=s[j,]
    }
    #K-fold CV
    else {
      training=s%>%filter(part!=j)
      test=s%>%filter(part==j)
    }
    
    #make training set that has to undergo inner CV. So when using 1CV in inner loop, this set is further divided in training and validation set.
    training_X=training[,1:ncol(X)]%>%as.matrix()
    training_c=training$c%>%as.matrix()
    
    #determine optimum number of LVs with inner CV
    lv=LV_select_r(X=training_X,k=sK,c=training_c, A=A, method=method,r=r)
    
    #make model on the whole training set (training+validation)
    model<-plsda_function(training_X,training_c,A=lv)
    
    #make test X needed for prediction function
    test_X=test[,1:ncol(X)]%>%as.matrix()
    
    #needed for accuracy calculation (acc2, so to calculate the overall accuracy value of the whole dataset, so not a acc value for every test part of the data set, but one acc value for all test parts together.
    c_true<-c_true%>%append(test$c) #this 'appending' is also used in 1CV. See that function if you want explanation.
    
    #make test true class column needed for accuracy calculation (for every test part a acc value)
    test_c=test$c%>%as.matrix()
    
    #make prediction of the independent test set
    pred<-prediction(test_X,model)
    
    #pred is still in continuous numbers. Change these to classes with a threshold.
    predclass<-ifelse(pred>threshold, 1, -1)
  
    c_hat_abs<-c_hat_abs%>%append(pred)
    
    #store predclass. Needed to calculate the total acc of all test parts together
    c_hat<-c_hat%>%append(predclass)
    
    }
  #specificity calculation
  sp=specificity(pred=c_hat_abs,c=c_true,threshold=threshold)
  
  #accuracy calculation
  acc=acc(c=c_true,pred=c_hat_abs)
  
  #sensitivity calculation
  sn=sensitivity(pred=c_hat_abs,c=c_true,threshold=threshold)
  
  #calculate RMSEP
  RMSEP=RMSE(pred=c_hat_abs,c=c_true)
  
  #make list for the output. 
  list=list(acc2=acc, RMSEP=RMSEP,sp=sp,sn=sn) #acc is named acc2, to pretent miscommunications with the functions acc().
  
  return(list)}

```

```{r}
#repeated DCV function
rDCV<-function(X,c,dK=5,sK=5,p=0,A=20,r,rlv=1, method='RMSECV',mc=TRUE){
  #make vector to be able to store RMSEP and other metrics resulting from DCV
  RMSEPs=vector()
  ACCs=vector()
  SNs=vector()
  SPs=vector()
  
  #for loop for different repeats.
  for (i in 1:r){
    #constant results
    ss=i
    
    #get the performance in 4 different metrics of this loop and store (RMSEP, Accuracy, Sensitivity, Specificity)
    dcv=DCV(X=X,c=c,dK=dK,sK=sK,p=p,ss=ss,A=A,r=rlv,method=method,mc=mc)
    RMSEPs=RMSEPs%>%append(dcv$RMSEP)
    ACCs=ACCs%>%append(dcv$acc2)
    SNs=SNs%>%append(dcv$sn)
    SPs=SPs%>%append(dcv$sp)
  }
  #make list for the output. Means and standard deviations of RMSEP, Accuracy, sensititivy and specificity are stored. 
  list=list(RMSEP_mean=RMSEPs%>%mean(),RMSEP_sd=RMSEPs%>%sd(),ACC_mean=ACCs%>%mean(),ACC_sd=ACCs%>%sd(),SN_mean=SNs%>%mean(),SN_sd=SNs%>%sd(),SP_mean=SPs%>%mean(),SP_sd=SPs%>%sd())
  return(list)
}
```

```{r}
#function to give the error estimation resulting from 1CV. SO based on the validation set. See CV() function for explanation of the lines. This function is very  similar, except that the error estimation with a certain number of components is the output.
CVpred=function(X,c,Aopt,ss=1,k){
  #X: data set
  #c: class label vector belonging to X
  #Aopt: the optimal number of components to use in model building based on X
  #ss: set.seed. 
  #k:k in k-fold CV.
  set=as.data.frame(X) 
  c=as.matrix(c)
  c_mean=mean(c)
  set.seed(ss)
  set$part<-partition(k, c)
  set$c<-c
  c_true=vector()
  c_pred=vector()
  accus=vector()
  for (i in 1:k){
    if (k==nrow(X)){
      training=set[-i,]
      test=set[i,]
    } 
    else {
      training=set%>%filter(part!=i)
      test=set%>%filter(part==i)
    }
    training_X=training[,1:ncol(X)]%>%as.matrix()
    training_c=training$c%>%as.matrix()
    test_X=test[,1:ncol(X)]%>%as.matrix()
    test_c=test$c%>%as.matrix()
    c_true=c_true%>%append(test_c)
    model=plsda_function(training_X,training_c,Aopt)
    pred=prediction(X=test_X,m=model)
    c_pred=c_pred%>%append(pred)
  }
  accu=acc(c=c_true,pred=c_pred)
  return(accu)
}
```

```{r}
#function for repeated CVpred(). 
r1cv=function(X,c,Aopt,r,k){
  #Same arguments as CVpred().
  v=vector() #to store output of CVpred() of different repeats.
  for (i in 1:r){
    v=v%>%append(CVpred(X=X,c=c,Aopt=Aopt,ss=i,mc=mc,k=k))
  }
  return(mean(v))
}
```

```{r}
#function to make a percentile CI with bootstrapping.
BTS_percentileCI=function(X,c,r=499,Aopt,a=0.05){
  #X: data set
  #c: class label belonging to X
  #r: number of bootstrap repeats
  #Aopt: optimal number of components. (often result from LV_select_r())
  #a: alpha value. 0.05 when 95% CI is wanted etc.
  
  aci=a*(r+1) #aci: which observation should be taken as boundary. With r=499 and a=0.05 this is the 25th observation, then aci=25. 
  VIPb=matrix(NA,nrow=r,ncol=ncol(X)) #to store VIP values of the bootstrap repeats
  SRb=matrix(NA,nrow=r,ncol=ncol(X)) #to store SR values of the bootstrap repeats.
  for (b in 1:r){ #calculate VIP and SR and store it. For every bootstrap repeat.
    bts=BTS_sampling(X=X,c=c,ss=b)
    model=plsda_function(X=bts$BTS_X,c=bts$BTS_c,A=Aopt)
    vipb=VIP(X=bts$BTS_X,model=model)
    VIPb[b,]=vipb$VIP
    srb=SR(X=bts$BTS_X,model=model)
    SRb[b,]=srb$SR
  }
  low=matrix(NA,ncol=ncol(X),nrow=2) #first row: VIP, second row: SR. The lower bound will be stored, as the CI is one-sided, so it has no upper bound.
  for (m in 1:ncol(X)){
    sor=sort(VIPb[,m],decreasing = FALSE)
    low[1,m]=sor[aci]
    sor=sort(SRb[,m],decreasing = FALSE)
    low[2,m]=sor[aci]
  }
  return(low)
}
```

```{r}
#function to make random CI of VIP/SR based on permutation.
randomCI=function(X,c,r,Aoptdf,ss){
  #X: The data set
  #c: the class label vector belonging to X
  #r: number of repeats. 
  #Aoptdf: the dataframe wit the Aopt per random label per small set. SHould be calculated before this function starts. When it is not available, change the line that determines Aopt.
  #ss: set.seed
  vipm=matrix(NA,ncol=ncol(X),nrow=r) #to store vip scores of different repeats
  srm=matrix(NA,ncol=ncol(X),nrow=r) #to store sr of different repeats.
  for (j in 1:r){ 
    cr=c_random(c=c,ss=j)
    Aopt=Aoptdf[ss,j] #when no Aoptdf is already calculated: use LV_select_r(). It is the Aopt calculated based on the random label.
    model=plsda_function(X=X,c=cr,A=Aopt)
    vip=VIP(X=X,model=model)
    vipm[j,]=vip$VIP
    sr=SR(X=X,model=model)
    srm[j,]=sr$SR
  }
  high=matrix(NA,ncol=ncol(X),nrow=2) #first row: VIP, second row: SR. It is a one sided CI. You only want the upper bound. This is stored in this matrix. 
  for (m in 1:ncol(X)){
    sor=sort(vipm[,m],decreasing = TRUE)
    high[1,m]=sor[5] #5 is hardcoded for 99 repeats.
    sor=sort(srm[,m],decreasing = TRUE)
    high[2,m]=sor[5] #5 is hardcoded for 99 repeats. 
  }
  return(high)
}
```


```{r}
#function used for illustration investigation. Same as CV, but then predicted c converted to a predicted class is the output. See CV() function for more information about every line.
CVpredclass=function(X,c,Aopt,ss=1,k,mc=TRUE){
  set=as.data.frame(X)
  c=as.matrix(c)
  c_mean=mean(c)
  set.seed(ss)
  set$part<-partition(k, c)
  set$c<-c
  c_true=vector()
  c_pred=vector()
  accus=vector()
  for (i in 1:k){
    if (k==nrow(X)){
      training=set[-i,]
      test=set[i,]
    } 
    else {
      training=set%>%filter(part!=i)
      test=set%>%filter(part==i)
    }
    training_X=training[,1:ncol(X)]%>%as.matrix()
    training_c=training$c%>%as.matrix()
    test_X=test[,1:ncol(X)]%>%as.matrix()
    test_c=test$c%>%as.matrix()
    c_true=c_true%>%append(test_c)
    model=plsda_function(training_X,training_c,Aopt,mc=mc)
    pred=prediction(X=test_X,m=model,mc=mc)
    c_pred=c_pred%>%append(pred)
  }
  return(data.frame(c=c_true,pred=c_pred))
}
```

