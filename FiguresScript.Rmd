---
title: "Final"
author: "Sjoerd Garssen"
date: "13/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this document all the figures of the thesis and the SI are made. The selfwritten functions are written in the other script document. Thus, the other script needs to be fully runned before executing this script. Note that some chunks can be used to make multiple models when running it two times with small adaptions in the script. When this yields for a chunk, then this is always indicated in the comments of that chunk. 

```{r}
#open the data. In my computer it was stored in the M-drive as 'mamma.csv'.
setwd("M:")
raw_data=read.csv('mamma.csv', skip=8)
c=raw_data[-c(1:3),2]
c=ifelse(c=='Cancer',1,-1) #Cancer=1, healthy=-1
X=raw_data[,c(10:138)] #get the 129 numerical variables
X=X[-(1:3),] #delete 3 not needed rows (contained info about variable, not observations)
X=X%>%apply(2,as.numeric)
```

```{r}
#In this Chunk the Model Selection plot is made. This is figure 4 in the paper.
#-------------
#store Aopt values for later usage
Aopti=data.frame(ss=1:100,RMSECV=NA,DQ2=NA,NMC=NA,BTS=NA, BTSacc=NA) #ss indicates small set number, the others the method for model selection.

#Store model selection performance in 4 different metrics with 100 small sets. In the paper only Accuracy is used.
dfACC=data.frame(ss=1:100,RMSECV=NA,DQ2=NA,NMC=NA,BTS=NA, BTSacc=NA)
for (i in 1:100){ #repeat whole process for 100 small sets
  #make small set by the self-written smallsample() function.
  small=smallsample(X=X,c=c,n=50,ss=i,bal=1)
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  
  #column index for storing the performance values for every model selection method. 
  index=2
  
  #execute model selection and calculate performance of model with the respective Aopt on the large data set. Do this for every model selection method.
  for (j in c('RMSECV','DQ2','NMC','BTS','BTSacc')){
    #1CV methods. Use r=20 (repeats) and k=25 (in k-fold 1CV). 
    if (j=='RMSECV' || j=='DQ2' || j=='NMC'){
      r=20
      for (k in c(25)){ #this could be extended to multiple values for k in this way.
        Aopt=LV_select_r(X=smallx,k=k,c=smallc,A=15,r=r,method=j) #Calculate Aopt with the self-written LV_select_r() function. 
        Aopti[i,index]=Aopt #Store Aopt for later usage
        model=plsda_function(X=smallx,c=smallc,A=Aopt) #make model with small set and Aopt with the self-written plsda_function() function.
        pred=prediction(X=X,m=model) #predict large set with the self-written prediction() function.
        #next lines: calculate Accuracy with self-written function and store in data frames.
        dfACC[i,index]=acc(c=c,pred=pred,threshold=0)
        index=index+1
      }}

    #BTS 0.632 error.Both RMSE and Accuracy as error metric.
    else if (j%in%c('BTS','BTSacc')){
      r=500 #500 bootstrap repeats
      Aopt=LV_select_r(X=smallx,c=smallc,A=15,r=r,method=j) #select Aopt
      Aopti[i,index]=Aopt #store Aopt
      model=plsda_function(X=smallx,c=smallc,A=Aopt)#make model
      pred=prediction(X=X,m=model) #predict population
      #next lines: calculate and store acc
      dfACC[i,index]=acc(c=c,pred=pred,threshold=0)
      index=index+1
    }
    print(j) #print which method is done, to keep updated how far this script is with running. 
  }
  print(paste('set done:',i)) #print when a small set is completely done, to keep updated how far this script is with running.
}
#from here: make plots. In the next lines: remove the first column, as this column indicates the small set number (ss). Then calculate the mean and sd accuracy per model selection method
dfACC_m=dfACC[,-1]%>%apply(2,mean)
dfACC_s=dfACC[,-1]%>%apply(2,sd)

#reference performance. 5 times repeated 5-fold double cross validation based on NMC. Inner CV consisted of 10 times repeated 5-fold 1CV. Maximum number of components to consider (A): 25. 
rdcv=rDCV(X=X,c=c,dK=5,sK=5,A=25,r=5,rlv=10,method='NMC')

#make method vs accuracy plot (fig. 3)
plotacc=data.frame(method=c('RMSECV','DQ2','NMC','RMSE','Accuracy','Reference'),colour=c(rep('1CV',3),'Bootstrap','Bootstrap','Other'),acc=c(dfACC_m,rdcv$ACC_mean),sd=c(dfACC_s,rdcv$ACC_sd))%>%ggplot(aes(x=reorder(method,acc),y=acc,ymin=acc-sd,ymax=acc+sd,col=colour))+geom_point()+geom_errorbar()+labs(y='Accuracy (%)',x='Metric',col='Method')+ theme(legend.text=element_text(size=18),axis.text=element_text(size=18),legend.title=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18))
```

```{r}
#in this and the next Chunks, the predictive performance plot (figure 5 in thesis) is made. 
#This chunk needs to be run two times. First run this chunk, then make the small adaptions seen in the comments, and then run this chunk again. Hereafter, the next chunk makes figure 5.  
#--------

#dataframe to store results
pp=data.frame(ss=1:50,FIT=NA,k5cv=NA,k25cv=NA,k50cv=NA,dcv2=NA,dcv5=NA,BTS=NA,ref=NA) #pp: predictive performance
c_r=c_random(c,ss=26) #Make random population. ss=26 is the set.seed(26) that leads to ± equal distribution of the random classes over both true classes.
for (i in 1:50){ #repeat whole process for 50 small sets (N=50).
  #make small sample
  small=smallsample(X=X,c=c,n=50,ss=i,bal=1) #instead of c=c, put c=c_r to use random data.
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  
  #Aopt selection. Uncomment the right line depending on using the true population or the random population. So one line needs to be uncommented, while the others needs to be commented:
  Aopt=Aopti[i,6] #Optimal number of components of the respective sets were already calculated in model selection and stored in Aopti. The 6th column is corresponding to BTSacc model selection. Uncomment when using true class label
  #Aopt=LV_select_r(X=smallx,c=smallc,A=Aopt,r=50,method='BTSacc') #uncomment when using random class label.
  
  #calculate FIT predictive performance
  model=plsda_function(X=smallx,c=smallc,A=Aopt) #make model on small set with Aopt calculated/chosen in the lines above.
  pred=prediction(X=smallx,m=model) #predict small set
  pp[i,2]=acc(c=smallc,pred=pred) #store accuracy of prediction.
  print('FIT done') #to be updated on how far this script is with running.
  
  #calculate predictive performance resulting from r1CV with different K-folds.
  pp[i,3]=r1cv(X=smallx,c=smallc,Aopt=Aopt,r=5,k=5) 
  pp[i,4]=r1cv(X=smallx,c=smallc,Aopt=Aopt,r=5,k=25)
  pp[i,5]=r1cv(X=smallx,c=smallc,Aopt=Aopt,r=1,k=50) #one repeat (r=1), because K=50 is leave one out so every repeat is exactly the same.
  print('1CV done')
  
  #calculate DCV with different K's.
  dv_output=rDCV(X=smallx,c=smallc,dK=5,A=15,r=3,rlv=50,method='BTSacc') #store output of rDCV() function.
  pp[i,6]=dv_output$ACC_mean #get the right part of the output. 3 times repeated 5-fold DCV. Model Selection based on the bootstrap accuracy method with 50 repeats.
  dv_output=rDCV(X=smallx,c=smallc,dK=25,A=15,r=3,rlv=50,method='BTSacc') #3 times repeated leave two out DCV. Model selection based on BTS acc with 50 repeats. 
  pp[i,7]=dv_output$ACC_mean
  print('DCV done') #To be updated how far this script is with running.
  
  #calculate bootstrap
  bts=BTS_pred2(X=smallx,c=smallc,r=200,A=Aopt) #store output of bootstrap predictive performance function BTS_pred2().
  pp[i,8]=bts$accmean #get the right part of the output of the function.
  print('Bootstrap done') #to be updated 
  
  #reference value. Model made on 838-50 samples and tested on the 50 samples. So it is the accuracy that should be maximally obtained with this predictive performance investigation.
  X_=small[['large']][['Xlarge']] #the remainder of the population after drawing samples into a small set
  c_=small[['large']][['clarge']]
  pred=prediction(X=X_,m=model) #predict small set
  pp[i,9]=acc(c=c_,pred=pred) #store the accuracy of this prediction as reference (9th column). THis column is later needed for other figures too. 
  print('reference done') #to be update on how far this script is with running
  
  
  print(i) #to be updated in how far this script is in running
}
#uncommment the right line.
pp_true=pp #uncomment when using the true population
#pp_random=pp #uncomment when using the random population

```

```{r}
#After running the previous chunk for both random and true populations, figure 4 can be made by this chunk.
ppr=data.frame(method=c('FIT','5-fold 1CV','25-fold 1CV','LOOCV','25-fold 2V','5-fold 2V','BTS','Reference'),acc=pp_random[,-1]%>%apply(2,mean),sd=pp_random[,-1]%>%apply(2,sd)) #get the results from random population in the right structure to make plot.

plot=data.frame(method=c('FIT','5-fold 1CV','25-fold 1CV','LOOCV','25-fold 2V','5-fold 2V','BTS','Reference'),acc=pp_true[,-1]%>%apply(2,mean),sd=pp_true[,-1]%>%apply(2,sd))%>%ggplot(aes(x=reorder(method,acc),y=acc,ymin=acc-sd,ymax=acc+sd,col='True label'))+geom_errorbar()+geom_point()+labs(x='Method',y='Accuracy (%)',col='')+geom_errorbar(data=ppr,aes(x=method,y=acc,ymin=acc-sd,ymax=acc+sd,col='Random label'))+geom_point(data=ppr,aes(x=method,y=acc,ymin=acc-sd,ymax=acc+sd,col='Random label'))+ theme(legend.text=element_text(size=18),axis.text=element_text(size=18),legend.title=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18),axis.text.x = element_text(angle = 45,hjust = 1))
```



```{r}
#In this chunk the threshold plots are made (figure 3 in thesis)
#This chunk needs to be ran two times. First run it, than make the small adaptions mentioned in the comments, then run the chunk again.
#-------

#make dataframes to store accuracies, specificities and sensitivities.
undf=data.frame(ss=1:20,acc0=NA,accdis=NA,accmcc=NA) #data frame (df) to store accuracy values of unbalanced data (un)-->undf. 
undf_sn=data.frame(ss=1:20,sn0=NA,sndis=NA,snmcc=NA) #to store sensitivity values
undf_sp=data.frame(ss=1:20,sp0=NA,spdis=NA,spmcc=NA) #to store specificity values

#make 20 small sets and calculate performance on large set.
for (i in 1:20){ #repeat process for 20 small sets.
  #make small set
  small=smallsample(X=X,c=c,n=50,ss=i,bal=4) #Change to bal=1 for balanced classes.
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  Aopt=LV_select_r(X=smallx,c=smallc,A=15,r=50,method='BTS') #select Aopt based on 50 times repeat bootstrap RMSE. 15 components is the max Aopt considered.
  model=plsda_function(X=smallx,c=smallc,A=Aopt) #make model based on small set
  pred=prediction(X=X,m=model) #predict whole population
  #values with threshold 0:
  undf[i,2]=acc(c=c,pred=pred,threshold=0)
  undf_sn[i,2]=sensitivity(c=c,pred=pred,threshold=0)
  undf_sp[i,2]=specificity(c=c,pred=pred,threshold=0)
  
  #values with threshold distribution based:
  predsmall=prediction(X=smallx,m=model)
  thresdis=threshold(c=smallc,pred=predsmall) #get the threshold based on distribution based on self-written threshold() function.
  undf[i,3]=acc(c=c,pred=pred,threshold=thresdis)
  undf_sn[i,3]=sensitivity(c=c,pred=pred,threshold=thresdis)
  undf_sp[i,3]=specificity(c=c,pred=pred,threshold=thresdis)
  
  #values with threshold between means of the two classes after mean centring c:
  undf[i,4]=acc(c=c,pred=pred,threshold=model$threshold)
  undf_sn[i,4]=sensitivity(c=c,pred=pred,threshold=model$threshold)
  undf_sp[i,4]=specificity(c=c,pred=pred,threshold=model$threshold)
  print(i)#to be updated in how far this script is with running.
}
#get means and sds of accuracy-,sensitivity-, specificity-values.
undf_accmean<-undf[,-1]%>%apply(2,mean)
undf_accsd<-undf[,-1]%>%apply(2,sd)
undf_snmean<-undf_sn[,-1]%>%apply(2,mean)
undf_snsd<-undf_sn[,-1]%>%apply(2,sd)
undf_spmean<-undf_sp[,-1]%>%apply(2,mean)
undf_spsd<-undf_sp[,-1]%>%apply(2,sd)

#make data frame used to make figure 3
undfsum<-data.frame(threshold=rep(c(0,'distribution based','between mean centred c means'),3),metric=rep(c('accuracy','specificity','sensitivity'),each=3),mean=c(undf_accmean,undf_spmean,undf_snmean),sd=c(undf_accsd,undf_spsd,undf_snsd))

#undfsum=undfsum%>%filter(threshold!='between mean centred c means') #uncomment when using class balance. Threshold of 0 is exactly the same as threshold between mean centred c means. 

plot<-undfsum%>%ggplot(aes(x=metric,y=mean,ymin=mean-sd,ymax=mean+sd,col=threshold))+geom_point()+geom_errorbar()+labs(y='Value (%)',col='Threshold Method',x='Metric')+ theme(legend.text=element_text(size=14),axis.text=element_text(size=15),legend.title=element_text(size=15),axis.title.x = element_text(size=15),axis.title.y = element_text(size=15)) #change plot name when repeating this chunk for class balance, otherwise the unbalanced classes plot will be overwritten.


```

```{r}
#in this chunk figure 2 (thesis) is made.
#-----


small=smallsample(X=X,c=c,n=400,ss=50,bal=1) #independent test set
testx=small[['small']][['Xsmall']]
testc=small[['small']][['csmall']]
X_=small[['large']][['Xlarge']]%>%as.matrix() #438 remaining samples.
c_=small[['large']][['clarge']]%>%as.matrix()
store=data.frame(ss=1:50,n10=NA,n20=NA,n30=NA,n40=NA,n50=NA,n70=NA,n100=NA,n200=NA) #data frame to store performances in accuracy
for (i in 1:50){
  index=2
  for (n in c(10,20,30,40,50,70,100,200)){ #all considered training set sizes
    #for every training set size a different maximum number of components will be considered. The max number of components equals the size of the smallest group - 1. The maximums are chosen based on this fact and on intuition, as for example 9 components for a 20 sample set is thought to be very high. 
    if (n==10){
      A=4
    }
    else if (n==20){
      A=8
    }
    else if (n<60){
      A=15
    }
    else if (n==70){
      A=25
    }
    else if (n==100){
      A=35
    }
    else if (n==200){
      A=70
    }
    
    small=smallsample(X=X_,c=c_,n=n,ss=i,bal=1) #get small set with the respective size in the for loop
    smallx=small[['small']][['Xsmall']]
    smallc=small[['small']][['csmall']]
    Aopt=LV_select_r(X=smallx,c=smallc,A=A,r=50,method='BTSacc') #calculate Aopt
    model=plsda_function(X=smallx,c=smallc,A=Aopt) #build a model
    pred=prediction(X=testx,m=model) #predict test set
    store[i,index]=acc(c=testc,pred=pred) #store accuracy
    index=index+1 #to store in the right column index.
    print(n) #to be updated how far this script is with running.
  }
  print(i) #to be updated how far this script is with running.
}
#make figure
plot=data.frame(size=c(10,20,30,40,50,70,100,200),acc=store[,-1]%>%apply(2,mean),sd=store[,-1]%>%apply(2,sd))%>%ggplot(aes(x=size,y=acc,ymin=acc-sd,ymax=acc+sd))+geom_line()+geom_errorbar()+labs(x='#samples in training set',y='Accuracy (%)')+ theme(axis.text=element_text(size=15),axis.title.x = element_text(size=20),axis.title.y = element_text(size=20))

```

```{r}
#in this chunk figure 7 and figure S5 can be made.
#This chunk needs to be run two times. First run it, then make the small adaptions mentioned in the comments, and then run again.
#-------

#get three truly important variables based on whole population. These variables are known to be the most important based on both SR and VIP.
model=plsda_function(X=X,c=c,A=9) #Aopt=9 is based on a model selection plot based on NMC of the whole population. Make model on whole population
vip=VIP(X,model) #calculate VIP of the model. Could also been SR, as the 3 truly important variables were also the 3 highest scoring variables based on SR.
sor=vip$VIP%>%sort(decreasing=TRUE,index.return=TRUE) #sort VIP values and store indices.
top3=colnames(X)[sor$ix[1:3]] #get the variable names of the truly imporant variables.

#here: calculate the Aopt for 100 random labels for all 50 small sets. In thus chunk only 99 random labels are used, but in a later chunk 100 random labels are needed. 
randomAopt=matrix(NA,nrow=50,ncol=99)
for (i in 1:50){
  small=smallsample(X=X,c=c,n=50,ss=i,bal=1) #change to n=100 to make the figure S5
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  for (r in 1:100){
    cr=c_random(c=smallc,ss=r)
    Aopt=LV_select_r(X=smallx,c=cr,A=15,r=50,method='BTSacc')
    randomAopt[i,r]=Aopt
  }
}

vipimp=data.frame(ss=rep(1:50,each=2),pos=rep(c('TP','FP'),50),randomCI=NA,doubleCI=NA,highest=NA) #store IMPortant variables (TPs etc.) based on VIP
srimp=data.frame(ss=rep(1:50,each=2),pos=rep(c('TP','FP'),50),randomCI=NA,doubleCI=NA,highest=NA) #store IMPortant variables based on SR.
for (i in 1:50){ #repeat for 50 sets.
  index=(i*2)-1 #get the right column index for the storage in vipimp and srimp.
  small=smallsample(X=X,c=c,n=50,ss=i,bal=1) #change to n=100 for SI figure 5
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  Aopt=Aopti[i,6] #uncomment when n=50
  #Aopt=LV_select_r(X=smallx,c=smallc,A=20,r=50,method='BTSacc') #uncomment when n=100
  
  #normal: simply detect the 3 highest scoring variables
  model=plsda_function(X=smallx,c=smallc,A=Aopt)
  vip=VIP(X=smallx,model=model)
  vip=vip$VIP
  sr=SR(X=smallx,model=model)
  sr=sr$SR
  sor=sort(vip,decreasing=TRUE,index.return=TRUE)
  viph=sor$ix[1:3]
  sor=sort(sr,decreasing=TRUE,index.return=TRUE)
  srh=sor$ix[1:3]
  
  #calculate the true CI
  ci=BTS_percentileCI(X=smallx,c=smallc,r=499,Aopt=Aopt)
  vipci=ci[1,] #first row indicates the VIP bound
  srci=ci[2,] #second row indicates the SR bound
  
  #calculate the random CI
  rCI=randomCI(X=smallx,c=smallc,r=99,Aoptdf=randomAopt,ss=i) 
  vipci_r=rCI[1,] #first row indicates the VIP bound
  srci_r=rCI[2,] #second row indicates the SR bound
  
  #calculate True Positive (TP) for each method
  #vip
  vipimp[index,3]=colnames(smallx)[vip>vipci_r]%in%top3%>%sum() #random CI method
  vipimp[index,4]=colnames(smallx)[vipci>vipci_r]%in%top3%>%sum() #double CI method
  vipimp[index,5]=colnames(smallx)[viph]%in%top3%>%sum() #highest 3 variables method
  #sr
  srimp[index,3]=colnames(smallx)[sr>srci_r]%in%top3%>%sum() #random CI method
  srimp[index,4]=colnames(smallx)[srci>srci_r]%in%top3%>%sum() #double CI method
  srimp[index,5]=colnames(smallx)[srh]%in%top3%>%sum() #highest 3 variables method
  index2=index+1
  
  #calculate False Positives for each method
  #vip
  vipimp[index2,3]=(vip>vipci_r)%>%sum()-vipimp[index,3] #random CI method
  vipimp[index2,4]=(vipci>vipci_r)%>%sum()-vipimp[index,4] #double CI method
  vipimp[index2,5]=3-vipimp[index,5] #highest 3 var method
  #sr
  srimp[index2,3]=(sr>srci_r)%>%sum()-srimp[index,3] #random CI method
  srimp[index2,4]=(srci>srci_r)%>%sum()-srimp[index,4] #double CI method
  srimp[index2,5]=3-srimp[index,5] #highest 3 var method
  print(i) #to be updated on how far this chunk is with running.
}

imp=as.data.frame(vipimp%>%pivot_longer(c('randomCI','doubleCI','highest'),names_to='method',values_to='amount')%>%group_by(method,pos)%>%summarise(sum=sum(amount))) #get the right data frame structure to make the figure
fns=data.frame(method=c('doubleCI','highest','randomCI'),pos=rep('FN',3),sum=150-(vipimp%>%pivot_longer(c('randomCI','doubleCI','highest'),names_to='method',values_to='amount')%>%group_by(method,pos)%>%summarise(sum=sum(amount))%>%filter(pos=='TP')%>%ungroup()%>%select(-c('method','pos')))) #calculate the False Negatives
imp=imp%>%rbind(fns) #store the FNs also in the data frame
plotvip=imp%>%ggplot(aes(x=method,y=sum,fill=pos))+geom_bar(stat='identity',position=position_dodge())+labs(fill='',y='#Variables',x='Method')+scale_x_discrete(labels = c('double CI','Highest 3','random CI'))+
  geom_text(aes(label=sum), vjust=0, color="black",
            position = position_dodge(0.9), size=3.5)+theme(legend.text=element_text(size=15),axis.text=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18)) #make plot based VIP. When repeating this chunk for the second time, than change the name of the plot, otherwise it will be overwritten

#below: exactly the same as above, but then based on SR.
imp=as.data.frame(srimp%>%pivot_longer(c('randomCI','doubleCI','highest'),names_to='method',values_to='amount')%>%group_by(method,pos)%>%summarise(sum=sum(amount)))
fns=data.frame(method=c('doubleCI','highest','randomCI'),pos=rep('FN',3),sum=150-(srimp%>%pivot_longer(c('randomCI','doubleCI','highest'),names_to='method',values_to='amount')%>%group_by(method,pos)%>%summarise(sum=sum(amount))%>%filter(pos=='TP')%>%ungroup()%>%select(-c('method','pos'))))
imp=imp%>%rbind(fns)
plotsr=imp%>%ggplot(aes(x=method,y=sum,fill=pos))+geom_bar(stat='identity',position=position_dodge())+labs(fill='',y='#Variables',x='Method')+scale_x_discrete(labels = c('double CI','Highest 3','random CI'))+
  geom_text(aes(label=sum), vjust=0, color="black",
            position = position_dodge(0.9), size=3.5)+theme(legend.text=element_text(size=15),axis.text=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18)) #make plot based on SR. When repeating this chunk for the second time, than change the name of the plot, otherwise it will be overwritten

#store the Aopt's of random labels needed to make other figures later in this markdown (i.e. figure 6):
randomAopt_n50=randomAopt #uncomment when using n=50
#randomAopt_n100=randomAopt #uncomment when using n=100

```

```{r}
#in this Chunk figure 4b (thesis) is made
#-------
mean=Aopti[,-1]%>%apply(2,mean) #get mean Aopt's per method. Remove first column as this is indicating the small set numbers
sd=Aopti[,-1]%>%apply(2,sd) #get standard deviations per method.
colnames(Aopti)=c('ss','RMSECV','DQ2','NMC','RMSE','Bangle','Accuracy')
plot=data.frame(method=colnames(Aopti[,-1]),col=c(rep('1CV',3),'Bootstrap','Bootstrap'),mean=mean,sd=sd)%>%ggplot(aes(x=reorder(method,mean),y=mean,ymin=mean-sd,ymax=mean+sd,col=col))+geom_errorbar()+geom_point()+labs(x='Metric',y='Optimal number of components',col='Method')+ theme(legend.text=element_text(size=18),axis.text=element_text(size=18),legend.title=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18)) #get plot
```

```{r}
#in this chunk figure 8 is made.
#------

#score plot graph (fig 7a)
small=smallsample(X=X,c=c,n=20,ss=1,bal=1) #draw small set
smallx=small[['small']][['Xsmall']]
smallc=small[['small']][['csmall']]
Aopt=LV_select_r(X=smallx,c=smallc,A=4,r=50,method='BTSacc') #select Aopt. 4 is considered as maximum number of components. 

model=plsda_function(X=smallx,c=smallc,A=Aopt) #make model to get the scres
sc=model$Scores%>%as.data.frame() #subtract the scores from the previous output
sc$V3=smallc #add a column with the true classes needed to give the points in the score plot the right colour.
scoreplot=sc%>%ggplot(aes(x=V1,y=V2,col=as.factor(V3)))+geom_point(size=2)+labs(x='Scores component 1',y='Scores component 2',col='Class')+theme(legend.text=element_text(size=15),axis.text=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18),legend.title=element_text(size=18)) #make score plot.

pred=data.frame(c=NA,pred=NA) #to store class predictions and true classes out of the r1CV (CVpredclass() function
for (r in 1:5){
  pred=pred%>%rbind(CVpredclass(X=smallx,c=smallc,Aopt=Aopt,ss=r,k=10))
}
pred=pred[-1,] #first row are NA's, that should be removed. These NA's exist due to the way the data.frame was made a few lines above.
plot=ggplot(pred,aes(x=pred,fill=as.factor(c)))+geom_density(alpha=0.5)+labs(fill='Class:',x='Prediction',y='Density')+xlim(-5,5)+theme(legend.text=element_text(size=15),axis.text=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18),legend.title=element_text(size=18)) #make distribution plot.
```

```{r}
#in this chunk figure S7 is made.
#Run this chunk two times. First run it, then make the one small adaption mentioned in a comment, then run it again. Make sure to change the plotname before running it for the second time.
#----- 

#Supl Info: distribution plots for a 50-sample set
small=smallsample(X=X,c=c,n=50,ss=1,bal=1) #chang to ss=11 for second 50-sample plot.
smallx=small[['small']][['Xsmall']]
smallc=small[['small']][['csmall']]
Aopt=LV_select_r(X=smallx,c=smallc,A=15,r=50,method='BTSacc') #select Aopt

model=plsda_function(X=smallx,c=smallc,A=Aopt)
acc(pred=prediction(X=X,m=model),c=c) #get the accuracy of predicting the whole population, to indicate the quality of the model.
pred=data.frame(c=NA,pred=NA)
for (r in 1:5){
  pred=pred%>%rbind(CVpredclass(X=smallx,c=smallc,Aopt=Aopt,ss=r,k=25)) #store predictions of validation sets
}
pred=pred[-1,] #remove first row as this is an NA due to the way the data frame is made two lines above.
ggplot(pred,aes(x=pred,fill=as.factor(c)))+geom_density(alpha=0.5)+labs(fill='True Class:',x='Prediction',y='Density')+xlim(-5,5)
```

```{r} 
#in this Chunk figure S1 is made.
#------

small=smallsample(X=X,c=c,n=50,ss=10,bal=1)
smallx=small[['small']][['Xsmall']]
smallc=small[['small']][['csmall']]
ms=r1cvperformance(X=smallx,c=smallc,k=25,A=15,r=20,method='RMSECV') #Get the RMSECV mean and sd for every considered number of components for the Model Selection plot (ms). 
plotrmsecv=ms$df%>%ggplot(aes(x=A,y=mean,ymin=mean-sd,ymax=mean+sd))+geom_errorbar()+geom_line()+geom_point()+labs(x='#Components',y='RMSECV') #figure A

ms=BTS_LV_select(X=smallx,c=smallc,r=500,A=15) #get the RMSE values from every repeat and every number of components till 15 based on the bootstrap
means=ms%>%apply(2,mean)
sds=ms%>%apply(2,sd)
plotbtsrmse=data.frame(A=1:15,means=means,sds=sds)%>%ggplot(aes(x=A,y=means,ymin=means-sds,ymax=means+sds))+geom_errorbar()+geom_line()+labs(x='#Components',y='0.632 Bootstrap RMSE')+geom_point() #figure B

```

```{r}
#This Chunk makes figure 6a.
#------

dfs=data.frame(ss=1:10,k5=NA,k25=NA,dv5=NA,dv25=NA,bts=NA) #DataFrame to Store p-values
for (i in 1:10){ #repeat process for 10 small sets
  small=smallsample(X=X,c=c,n=50,ss=i,bal=1) #draw small set
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  Aopt=Aopti[i,6] #get its Aopt
  
  #get predictive performances
  k5=r1cv(X=smallx,c=smallc,Aopt=Aopt,mc=TRUE,r=5,k=5) #5 times repeated 5-fold 1CV
  k25=r1cv(X=smallx,c=smallc,Aopt=Aopt,mc=TRUE,r=5,k=25) #5 times repeated 25-fold 1CV
  dcv=rDCV(X=smallx,c=smallc,dK=5,A=15,r=3,rlv=50,method='BTSacc',mc=TRUE) #3 times repeated 5-fold DV
  dcv5=dcv$ACC_mean #get right part of the output of rDCV() function.
  dcv=rDCV(X=smallx,c=smallc,dK=25,A=15,r=3,rlv=50,method='BTSacc',mc=TRUE) #3 times repeated 5-fold DV
  dcv25=dcv$ACC_mean
  bt=BTS_pred2(X=smallx,c=smallc,r=100,A=Aopt)
  bts=bt$accmean
  
  #to store predictive performances of permutation sets
  sum5=vector()
  sum25=vector()
  sumdcv5=vector()
  sumdcv25=vector()
  sumbts=vector()

  for (r in 1:100){ #repeat for 100 permutated labels
    cr=c_random(c=smallc,ss=r) #permutate label
    Aopt=randomAopt_n50[i,r] #this data frame is already made to get figure 6 and S6.
    
    #store predictive performances in the vectors made above:
    sum5=sum5%>%append(r1cv(X=smallx,c=cr,Aopt=Aopt,mc=TRUE,r=5,k=5)) 
    sum25=sum25%>%append(r1cv(X=smallx,c=cr,Aopt=Aopt,mc=TRUE,r=5,k=25))
    r5=rDCV(X=smallx,c=cr,dK=5,A=15,r=3,rlv=50,method='BTSacc',mc=TRUE)
    sumdcv5=sumdcv5%>%append(r5$ACC_mean)
    r5=rDCV(X=smallx,c=cr,dK=25,A=15,r=3,rlv=50,method='BTSacc',mc=TRUE)
    sumdcv25=sumdcv25%>%append(r5$ACC_mean)
    bt=BTS_pred2(X=smallx,c=cr,r=100,A=Aopt)
    sumbts=sumbts%>%append(bt$accmean)
    if (r%in%c(10,20,30,40,50,60,70,80,90)){
      print(r) #to be updated how far this script is with running
    }
  }
  #get H0-distributions and calculate and store p-values
  #5-fold r1cv
  mean=mean(sum5)
  sd=sd(sum5)
  dfs[i,2]=pnorm(q=k5,mean=mean,sd=sd,lower.tail = FALSE)
  #25-fold r1cv
  mean=mean(sum25)
  sd=sd(sum25)
  dfs[i,3]=pnorm(q=k25,mean=mean,sd=sd,lower.tail = FALSE)
  #5-fold rDV
  mean=mean(sumdcv5)
  sd=sd(sumdcv5)
  dfs[i,4]=pnorm(q=dcv5,mean=mean,sd=sd,lower.tail = FALSE)
  #25-fold rDV
  mean=mean(sumdcv25)
  sd=sd(sumdcv25)
  dfs[i,5]=pnorm(q=dcv25,mean=mean,sd=sd,lower.tail = FALSE)
  #bootstrap
  mean=mean(sumbts)
  sd=sd(sumbts)
  dfs[i,6]=pnorm(q=bts,mean=mean,sd=sd,lower.tail = FALSE)
  print(i) #to be updates how far this script is with running
}

#make figure 5a (thesis)
ScaleFactor=0.005 #to be able to get the right axis in the right scale.
#pp_true was already earlier made. The 9th column shows how good a small set can be predicted with the remaining 788 samples from the whole population. Scalefactor is needed for the right axis.

plot=dfs%>%pivot_longer(cols=c('5-fold 1CV','25-fold 1CV','5-fold DV','25-fold DV','Bootstrap'),names_to='method',values_to = 'value')%>%ggplot(aes(x=ss,y=value,col=method))+geom_line()+geom_hline(yintercept=0.05,linetype='dashed')+labs(y='p-value',x='small set',col='Method')+scale_x_discrete(limits=c(1:10))+geom_line(data=pp_true[1:10,],mapping=aes(x=ss,y=ref*scaleFactor,col='Reference'),col='orange',linetype='longdash')+scale_y_continuous(name='p-value',sec.axis = sec_axis(~./scaleFactor,name='Accuracy (%)'))+theme(axis.title.y.left=element_text(color="black"),axis.text.y.left=element_text(color="black"),axis.title.y.right=element_text(color="orange"),axis.text.y.right=element_text(color="orange"),legend.text=element_text(size=18),axis.text=element_text(size=18),legend.title=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18))+labs(x='Small set',col='')

```

```{r}
#Chunk to make figure 6b and figure S4.
#This chunk should be run two times to make both figures. First run it and save the figure, than make the adaptions mentioned in the comments, and then run it again.
#------

dfs=data.frame(ss=1:25,k5=NA,bts=NA) #data frame to store p-values
for (i in 1:25){ #repeat process for 25 sets
  small=smallsample(X=X,c=c,n=50,ss=i,bal=1) #uncomment when using n=50
  #small=smallsample(X=X,c=c,n=100,ss=i,bal=1) #uncomment when using n=100
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]

  Aopt=Aopti[i,6] #uncomment when n=50
  #Aopt=LV_select_r(X=smallx,c=smallc,A=20,r=50,method='BTSacc') #uncomment when n=100
  k5=r1cv(X=smallx,c=smallc,Aopt=Aopt,mc=TRUE,r=5,k=5)
  bt=BTS_pred2(X=smallx,c=smallc,r=100,A=Aopt)
  bts=bt$accmean #get right part of the BTS_pred2() output.
  
  #make vectors to store random accuracies as is done in the chunk to make figure 5a.
  sum5=vector()
  sum25=vector()
  sumbts=vector()
  
  for (r in 1:100){ #100 random labels
    cr=c_random(c=smallc,ss=r)
    Aopt=randomAopt_n50[i,r] #This data frame is already made when generating figure 6 and figure S6. Uncomment this line when n=50 in small sets (figure 5b). 
    #Aopt=randomAopt_n100[i,r] #uncomment this line when n=100 (figure S4)
    
    #store predictive performances of permutated labels
    sum5=sum5%>%append(r1cv(X=smallx,c=cr,Aopt=Aopt,mc=TRUE,r=5,k=5))
    bt=BTS_pred2(X=smallx,c=cr,r=100,A=Aopt)
    sumbts=sumbts%>%append(bt$accmean)
  } 
  #get the p-values based on normal H0-distributions
  #5-fold r1cv
  mean=mean(sum5)
  sd=sd(sum5)
  dfs[i,2]=pnorm(q=k5,mean=mean,sd=sd,lower.tail = FALSE)
  #bootstrap
  mean=mean(sumbts)
  sd=sd(sumbts)
  dfs[i,3]=pnorm(q=bts,mean=mean,sd=sd,lower.tail = FALSE)
  print(i) #to be updates how far this chunk is with running
}
#get figure 5a. Comment the two lines when using N=100 for figure S4.
colnames(dfs)=c('ss','5-fold 1CV','Bootstrap') 
plot=dfs%>%pivot_longer(cols=c('5-fold 1CV','Bootstrap'),names_to = 'method',values_to = 'value')%>%ggplot(aes(x=ss,y=value,col=method))+geom_line()+geom_hline(yintercept=0.05,linetype='dashed')+labs(y='p-value',x='small set',col='Method')+scale_x_discrete(limits=c(1:25))+geom_line(data=pp_true[1:25,],mapping=aes(x=ss,y=ref*scaleFactor,col='Reference'),col='orange',linetype='longdash')+scale_y_continuous(name='p-value',sec.axis = sec_axis(~./scaleFactor,name='Accuracy (%)'))+theme(axis.title.y.left=element_text(color="black"),axis.text.y.left=element_text(color="black"),axis.title.y.right=element_text(color="orange"),axis.text.y.right=element_text(color="orange"),legend.text=element_text(size=18),axis.text=element_text(size=13),legend.title=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18))+labs(x='Small set',col='')

#get figure S4. Uncomment when using N=100, for figure S4. Comment the all the lines from here when using N=50.
dfref=data.frame(ss=1:25,ref=NA)
for (i in 1:25){ #get values for the dashed orange curve. So calculate how the small sample sets (N=100) can be predicited maximally as independent test set.
  small=smallsample(X=X,c=c,n=100,ss=i,bal=1) 
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  X_=small[['large']][['Xlarge']] 
  c_=small[['large']][['clarge']]
  Aopt=LV_select_r(X=X_,c=c_,A=20,r=100,method='BTSacc')
  model=plsda_function(X=X_,c=c_,A=Aopt)
  pred=prediction(X=smallx,m=model)
  dfref[i,2]=acc(c=smallc,pred=pred)
}

plot=dfs%>%pivot_longer(cols=c('5-fold 1CV','Bootstrap'),names_to = 'method',values_to = 'value')%>%ggplot(aes(x=ss,y=value,col=method))+geom_line()+geom_hline(yintercept=0.05,linetype='dashed')+labs(y='p-value',x='small set',col='Method')+scale_x_discrete(limits=c(1:25))+geom_line(data=dfref[1:25,],mapping=aes(x=ss,y=ref*scaleFactor,col='Reference'),col='orange',linetype='longdash')+scale_y_continuous(name='p-value',sec.axis = sec_axis(~./scaleFactor,name='Accuracy (%)'))+theme(axis.title.y.left=element_text(color="black"),axis.text.y.left=element_text(color="black"),axis.title.y.right=element_text(color="orange"),axis.text.y.right=element_text(color="orange"),legend.text=element_text(size=18),axis.text=element_text(size=13),legend.title=element_text(size=18),axis.title.x = element_text(size=18),axis.title.y = element_text(size=18))+labs(x='Small set',col='')

```

```{r}
#In this Chunk figure S5 is made.
#------

#make plot of SR and VIP of whole data set
Aopt=LV_select_r(X=X,c=c,A=20,r=100,method='BTSacc')
model=plsda_function(X=X,c=c,A=Aopt)
vip=VIP(X,model)
sr=SR(X,model)
scaleFactor <- max(vip$VIP) / max(sr$SR) #needed for the right axis
plot=vip%>%ggplot(aes(x=Var,y=VIP),col='blue')+geom_line(col='blue')+geom_line(aes(y=sr$SR*scaleFactor),col='red')+scale_y_continuous(name='VIP',sec.axis = sec_axis(~./scaleFactor,name='SR'))+theme(
    axis.title.y.left=element_text(color="blue"),
    axis.text.y.left=element_text(color="blue"),
    axis.title.y.right=element_text(color="red"),
    axis.text.y.right=element_text(color="red")
)+labs(x='Variable')
```

```{r}
#in this chunk figure S2 is made
#-----

#compare Aopt with 50 repeats to 500 repeats
rbts=data.frame(ss=1:50,r500=NA,r50=NA) #to store accuracy values of the different repeats in model selection. rbts: Repeated BooTStrap.
for (i in 1:50){ #repeat for 50 sets
  small=smallsample(X=X,c=c,n=50,ss=i,bal=1) #draw small set
  smallx=small[['small']][['Xsmall']]
  smallc=small[['small']][['csmall']]
  #500 repeats
  Aopt=Aopti[i,6]
  model=plsda_function(X=smallx,c=smallc,A=Aopt)
  pred=prediction(X=X,m=model)
  rbts[i,2]=acc(c=c,pred=pred) #store accuracy value of the prediction of the whole population
  
  #50 repeats
  Aopt=LV_select_r(X=smallx,c=smallc,A=15,r=50,method='BTSacc')
  model=plsda_function(X=smallx,c=smallc,A=Aopt)
  pred=prediction(X=X,m=model)
  rbts[i,3]=acc(c=c,pred=pred)
  print(i)
}
rbts%>%pivot_longer(cols=c('r500','r50'),values_to = 'value',names_to = 'repeats')%>%group_by(repeats)%>%summarise(sd=sd(value),mean=mean(value))%>%ggplot(aes(x=repeats,y=mean,ymin=mean-sd,ymax=mean+sd))+geom_errorbar()+labs(x='Number of bootstrap repeats',y='Accuracy')+geom_point() #gives the good structure to the data frame to be able to make the figure
colnames(rbts)=c('ss','500','50') #give right column names
rbts%>%pivot_longer(cols=c('500','50'),values_to = 'value',names_to = 'repeats')%>%ggplot(aes(x=ss,y=value,col=repeats))+geom_line()+labs(x='small set',y='Accuracy',col='number of repeats')
```
